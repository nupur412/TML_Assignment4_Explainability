{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements the [Assignment 4](https://github.com/sprintml/tml_2024/blob/main/Assignment4.pdf) - Task 2 of Trustworthy Machine Learning course offered in the Summer Semester 2024 at the Saarland University. This task focuses on obtaining annotations on 10 ImageNet images using [LIME (Local Interpretable Model Agnostic Explainations)](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20images%20-%20Pytorch.ipynb) technique and explaining the the predictions made by Resnet 50 model. The report analyzing the results of this task can be accessed [here](https://github.com/nupur412/TML_Assignment4_Explainability/blob/main/TML_Task_2_Report.pdf)"
      ],
      "metadata": {
        "id": "yhBnln0YRsrf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_q2uKNloqEW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os, json\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY9tenu-8NBC"
      },
      "outputs": [],
      "source": [
        "imgs = ['/content/n02098286_West_Highland_white_terrier.JPEG', '/content/n02018207_American_coot.JPEG', '/content/n04037443_racer.JPEG',\n",
        "        '/content/n02007558_flamingo.JPEG', '/content/n01608432_kite.JPEG', '/content/n01443537_goldfish.JPEG',\n",
        "        '/content/n01491361_tiger_shark.JPEG', '/content/n01616318_vulture.JPEG', '/content/n01677366_common_iguana.JPEG',\n",
        "        '/content/n07747607_orange.JPEG']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sections below follow the steps from the [LIME tutorial](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20images%20-%20Pytorch.ipynb)"
      ],
      "metadata": {
        "id": "kmoiI722fA9n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLegwzg48q9Q"
      },
      "outputs": [],
      "source": [
        "# applying transforms to the images\n",
        "inp_tensors = []\n",
        "logits_all_images = []\n",
        "\n",
        "def get_image(path):\n",
        "    with open(os.path.abspath(path), 'rb') as f:\n",
        "        with Image.open(f) as img:\n",
        "            return img.convert('RGB')\n",
        "\n",
        "def get_input_transform():\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "    transf = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    return transf\n",
        "\n",
        "def get_input_tensors(img):\n",
        "    transf = get_input_transform()\n",
        "    # unsqeeze converts single image to batch of 1\n",
        "    return transf(img).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pre-trained Resnet 50 model"
      ],
      "metadata": {
        "id": "LxJHP71AeSeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vseCmMjU8gXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c43d52-5929-41b6-8bb3-272225a4bb6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 135MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96qkGstb9JcQ"
      },
      "outputs": [],
      "source": [
        "# Load label texts for ImageNet predictions\n",
        "\n",
        "idx2label, cls2label, cls2idx = [], {}, {}\n",
        "with open(os.path.abspath('/content/imagenet_class_index.json'), 'r') as read_file:\n",
        "    class_idx = json.load(read_file)\n",
        "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
        "    cls2label = {class_idx[str(k)][0]: class_idx[str(k)][1] for k in range(len(class_idx))}\n",
        "    cls2idx = {class_idx[str(k)][0]: k for k in range(len(class_idx))}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following section, we obtain predictions for our images in the form of logits"
      ],
      "metadata": {
        "id": "xTn3_Wsfe4En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for img_path in imgs:\n",
        "    img = get_image(img_path)\n",
        "    img_t = get_input_tensors(img)\n",
        "    model.eval()\n",
        "    logits = model(img_t)\n",
        "    logits_all_images.append(logits)\n",
        "    inp_tensors.append(img_t)"
      ],
      "metadata": {
        "id": "h1eAUCZcdL6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing the logits through softmax to get the probabilities and class labels for top 5 predictions"
      ],
      "metadata": {
        "id": "DCJAAiA5UAMw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kjah2LfI9b3f",
        "outputId": "65ab5cb8-c130-4ab2-8d17-a890b2b2af76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((0.59718615, 203, 'West_Highland_white_terrier'), (0.009854398, 192, 'cairn'), (0.007290555, 153, 'Maltese_dog'), (0.0035356572, 194, 'Dandie_Dinmont'), (0.0032449872, 199, 'Scotch_terrier'))\n",
            "((0.48439145, 137, 'American_coot'), (0.05600774, 36, 'terrapin'), (0.020368645, 50, 'American_alligator'), (0.019573024, 136, 'European_gallinule'), (0.009864727, 135, 'limpkin'))\n",
            "((0.19987065, 817, 'sports_car'), (0.121991895, 751, 'racer'), (0.08636557, 479, 'car_wheel'), (0.07842865, 656, 'minivan'), (0.033425745, 436, 'beach_wagon'))\n",
            "((0.5576971, 130, 'flamingo'), (0.0029334582, 1, 'goldfish'), (0.0021150883, 100, 'black_swan'), (0.0009986997, 144, 'pelican'), (0.0008772656, 185, 'Norfolk_terrier'))\n",
            "((0.16977954, 129, 'spoonbill'), (0.11734087, 94, 'hummingbird'), (0.025400713, 989, 'hip'), (0.022102358, 12, 'house_finch'), (0.022069933, 716, 'picket_fence'))\n",
            "((0.56875235, 1, 'goldfish'), (0.0054592513, 0, 'tench'), (0.0032096482, 393, 'anemone_fish'), (0.0029346086, 392, 'rock_beauty'), (0.0021886125, 397, 'puffer'))\n",
            "((0.5988432, 3, 'tiger_shark'), (0.012200374, 2, 'great_white_shark'), (0.0033825315, 4, 'hammerhead'), (0.0018802831, 814, 'speedboat'), (0.0018788178, 13, 'junco'))\n",
            "((0.5279862, 23, 'vulture'), (0.023808397, 660, 'mobile_home'), (0.019813858, 127, 'white_stork'), (0.018409574, 497, 'church'), (0.00978666, 21, 'kite'))\n",
            "((0.40707582, 39, 'common_iguana'), (0.002994648, 46, 'green_lizard'), (0.001996828, 41, 'whiptail'), (0.0016421371, 38, 'banded_gecko'), (0.001397768, 43, 'frilled_lizard'))\n",
            "((0.33791485, 950, 'orange'), (0.021955049, 951, 'lemon'), (0.010794885, 942, 'butternut_squash'), (0.010678818, 809, 'soup_bowl'), (0.0050493684, 659, 'mixing_bowl'))\n"
          ]
        }
      ],
      "source": [
        "for logits in logits_all_images:\n",
        "  probs = F.softmax(logits, dim=1)\n",
        "  probs5 = probs.topk(5)\n",
        "  print(tuple((p,c, idx2label[c]) for p, c in zip(probs5[0][0].detach().numpy(), probs5[1][0].detach().numpy())))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next section, we define two separate transforms: (1) to take PIL image, resize and crop it (2) take resized, cropped image and apply whitening."
      ],
      "metadata": {
        "id": "rXpCAs_SgXef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKImYufj9iSk"
      },
      "outputs": [],
      "source": [
        "def get_pil_transform():\n",
        "    transf = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224)\n",
        "    ])\n",
        "\n",
        "    return transf\n",
        "\n",
        "def get_preprocess_transform():\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    return transf\n",
        "\n",
        "pill_transf = get_pil_transform()\n",
        "preprocess_transform = get_preprocess_transform()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the classification function that takes an array of perturbed images as input and producing probabilities for each class for each image as input"
      ],
      "metadata": {
        "id": "G9TAYwOygvTG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHxILLGB9ldZ"
      },
      "outputs": [],
      "source": [
        "def batch_predict(images):\n",
        "    model.eval()\n",
        "    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    batch = batch.to(device)\n",
        "\n",
        "    logits = model(batch)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    return probs.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the classification function on 10 ImageNet images"
      ],
      "metadata": {
        "id": "Wdx3277ehUcO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9EV9Xhg9m1B",
        "outputId": "b042d62f-dc44-4f31-c76a-78ddb43e4d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "203\n",
            "137\n",
            "817\n",
            "130\n",
            "129\n",
            "1\n",
            "3\n",
            "23\n",
            "39\n",
            "950\n"
          ]
        }
      ],
      "source": [
        "for img_path in imgs:\n",
        "    img = get_image(img_path)\n",
        "    test_pred = batch_predict([pill_transf(img)])\n",
        "    print(test_pred.squeeze().argmax())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following section imports lime in order to generate explainations for the obtained model predictions"
      ],
      "metadata": {
        "id": "EIoAXM96ijBi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y-i9Qnb97AJ",
        "outputId": "9b75c161-331b-4c0e-ea66-0edac95d31a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lime'...\n",
            "remote: Enumerating objects: 2389, done.\u001b[K\n",
            "remote: Total 2389 (delta 0), reused 0 (delta 0), pack-reused 2389\u001b[K\n",
            "Receiving objects: 100% (2389/2389), 21.41 MiB | 14.16 MiB/s, done.\n",
            "Resolving deltas: 100% (1600/1600), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/marcotcr/lime.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install lime"
      ],
      "metadata": {
        "id": "lLLkNki1W-hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRwnLasyE6y8"
      },
      "outputs": [],
      "source": [
        "from lime import lime_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtaining an explaination for model predictions for each of the 10 ImageNet data points"
      ],
      "metadata": {
        "id": "iZXHJH55iyuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explanations = []\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "for img_path in imgs:\n",
        "    img = get_image(img_path)\n",
        "    explanation = explainer.explain_instance(np.array(pill_transf(img)),\n",
        "                                         batch_predict,\n",
        "                                         top_labels=5,\n",
        "                                         hide_color=0,\n",
        "                                         num_samples=1000)\n",
        "    explanations.append(explanation)"
      ],
      "metadata": {
        "id": "FOtqRiYNObYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying mask on images and then finding which areas of the image encourage top prediction"
      ],
      "metadata": {
        "id": "oQaLcYfFjZaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.segmentation import mark_boundaries"
      ],
      "metadata": {
        "id": "9S8s9JgNOg3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks = []\n",
        "for id, explanation in enumerate(explanations):\n",
        "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
        "    img_boundry1 = mark_boundaries(temp/255.0, mask)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img_boundry1)\n",
        "    masks.append(mask)\n",
        "\n",
        "    plt.savefig(f'/content/output_image_with_boundaries{id}.png')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gTs5KLQsO-Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the obtained masks for every image, needed for task 4 of the assignment"
      ],
      "metadata": {
        "id": "wXvJYzt3jlWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, mask in enumerate(masks):\n",
        "    plt.figure()\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Save the mask image\n",
        "    save_path = os.path.join('/content/', f'limeMask_{i}.png')\n",
        "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)"
      ],
      "metadata": {
        "id": "v4KXS5vWXhtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In next section, we turn on areas that contribute against the top prediction by setting positive_only to False"
      ],
      "metadata": {
        "id": "7V1bsj2PkU8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for id, explanation in enumerate(explanations):\n",
        "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\n",
        "    img_boundry2 = mark_boundaries(temp/255.0, mask)\n",
        "    plt.imshow(img_boundry2)\n",
        "    plt.axis('off')\n",
        "    plt.savefig(f'/content/against_prediction_output_image_with_boundaries{id}.png')"
      ],
      "metadata": {
        "id": "-2neJh4VnshE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}